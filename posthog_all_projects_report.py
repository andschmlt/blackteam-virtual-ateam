#!/usr/bin/env python3
"""
PostHog All Projects Report Generator
Generates comprehensive analytics reports for all PostHog projects

Usage:
    python3 posthog_all_projects_report.py              # All projects
    python3 posthog_all_projects_report.py all          # All projects
    python3 posthog_all_projects_report.py lover.io     # Single domain
    python3 posthog_all_projects_report.py pokerology.com  # Single domain

Author: BlackTeam - DataViz + Insight
Date: January 16, 2026

Director Rule: All analysis reports MUST be generated as PDF in addition to markdown.
"""

import os
import sys
import json
import urllib.request
import urllib.error
from datetime import datetime
from pathlib import Path

# PDF Generation
try:
    from fpdf import FPDF
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("WARNING: fpdf not installed. PDFs will not be generated. Install with: pip install fpdf2")

# Configuration
POSTHOG_PERSONAL_API_KEY = os.environ.get("POSTHOG_PERSONAL_API_KEY")
POSTHOG_HOST = "https://us.i.posthog.com"
REPORTS_DIR = Path("/mnt/c/Users/andre/Desktop/Virtual ATeam/BlackTeam/projects/posthog-integration/reports/all_projects")


class AnalyticsPDF(FPDF if PDF_AVAILABLE else object):
    """PDF report generator for PostHog analytics"""

    def __init__(self):
        if not PDF_AVAILABLE:
            return
        super().__init__()
        self.set_auto_page_break(auto=True, margin=15)

    def header(self):
        self.set_font('Helvetica', 'B', 10)
        self.set_text_color(100, 100, 100)
        self.cell(0, 10, 'PostHog Analytics Report - Paradise Media', align='C', new_x="LMARGIN", new_y="NEXT")
        self.ln(2)

    def footer(self):
        self.set_y(-15)
        self.set_font('Helvetica', 'I', 8)
        self.set_text_color(128, 128, 128)
        self.cell(0, 10, f'Page {self.page_no()} | Generated by BlackTeam - DataViz + Insight', align='C')

    def chapter_title(self, title, min_space_after=50):
        """
        Add chapter title with optional page break if not enough space.
        DIRECTOR RULE: Ensure titles aren't orphaned at page bottom.
        """
        # Check if we have enough space for title + some content
        if self.get_y() + min_space_after > 270:
            self.add_page()

        self.set_font('Helvetica', 'B', 14)
        self.set_fill_color(41, 128, 185)
        self.set_text_color(255, 255, 255)
        self.cell(0, 10, title, fill=True, new_x="LMARGIN", new_y="NEXT")
        self.set_text_color(0, 0, 0)
        self.ln(3)

    def section_title(self, title, min_space_after=40):
        """
        Add section title with optional page break if not enough space.
        DIRECTOR RULE: Ensure titles aren't orphaned at page bottom.
        """
        # Check if we have enough space for title + some content
        if self.get_y() + min_space_after > 270:
            self.add_page()

        self.set_font('Helvetica', 'B', 11)
        self.set_text_color(41, 128, 185)
        self.cell(0, 8, title, new_x="LMARGIN", new_y="NEXT")
        self.set_text_color(0, 0, 0)
        self.ln(2)

    def body_text(self, text):
        self.set_font('Helvetica', '', 10)
        self.multi_cell(0, 5, text)
        self.ln(2)

    def add_table(self, headers, data, col_widths=None):
        """
        Add a table to the PDF with automatic page break if needed.
        DIRECTOR RULE: Tables MUST NEVER be split across pages.
        """
        if not data:
            return
        if col_widths is None:
            col_widths = [190 // len(headers)] * len(headers)

        # DIRECTOR RULE 2: Check if table fits on current page
        # Calculate table height: header (7) + rows (6 each) + margin (10)
        num_rows = min(len(data), 20)  # We limit to 20 rows
        table_height = 7 + (num_rows * 6) + 10

        # Page height is ~297mm, with margins ~15mm top/bottom, usable ~267
        # Current Y + table height should not exceed 270
        if self.get_y() + table_height > 270:
            self.add_page()

        # Header
        self.set_font('Helvetica', 'B', 9)
        self.set_fill_color(52, 73, 94)
        self.set_text_color(255, 255, 255)
        for i, header in enumerate(headers):
            self.cell(col_widths[i], 7, str(header)[:20], border=1, fill=True, align='C')
        self.ln()

        # Data
        self.set_font('Helvetica', '', 9)
        self.set_text_color(0, 0, 0)
        fill = False
        for row in data[:20]:  # Limit rows
            if fill:
                self.set_fill_color(236, 240, 241)
            else:
                self.set_fill_color(255, 255, 255)
            for i, cell in enumerate(row):
                self.cell(col_widths[i], 6, str(cell)[:30], border=1, fill=True, align='C')
            self.ln()
            fill = not fill
        self.ln(3)

    def add_metric_box(self, label, value, rating=None):
        self.set_font('Helvetica', 'B', 10)
        self.cell(70, 8, label + ":", border=0)
        self.set_font('Helvetica', '', 10)

        if rating:
            if rating == "Good":
                self.set_text_color(39, 174, 96)
            elif rating == "Needs Improvement":
                self.set_text_color(243, 156, 18)
            else:
                self.set_text_color(231, 76, 60)

        self.cell(50, 8, str(value), border=0)

        if rating:
            self.cell(40, 8, f"({rating})", border=0)
            self.set_text_color(0, 0, 0)

        self.ln()


def generate_pdf_report(project: dict, data: dict) -> str:
    """Generate PDF report for a project - Director Rule: MANDATORY"""
    if not PDF_AVAILABLE:
        print("WARNING: PDF generation skipped - fpdf not available")
        return None

    today = datetime.now().strftime("%B %d, %Y")

    total_events = data.get("total_events", 0)
    unique_users = data.get("unique_users", 0)
    sessions = data.get("sessions", 0)
    avg_events_session = round(total_events / sessions, 1) if sessions > 0 else 0

    web_vitals = data.get("web_vitals", {"lcp": 0, "cls": 0, "inp": 0})
    lcp = round(web_vitals.get("lcp", 0), 0)
    cls_val = round(web_vitals.get("cls", 0), 3)
    inp = round(web_vitals.get("inp", 0), 0)

    pdf = AnalyticsPDF()
    pdf.add_page()

    # Title
    pdf.set_font('Helvetica', 'B', 20)
    pdf.set_text_color(41, 128, 185)
    pdf.cell(0, 15, f'PostHog Analytics Report', align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.set_font('Helvetica', 'B', 16)
    pdf.set_text_color(0, 0, 0)
    pdf.cell(0, 10, project['name'], align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.set_font('Helvetica', '', 11)
    pdf.cell(0, 8, f'Report Date: {today} | Period: Last 7 Days', align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.cell(0, 6, f'Project ID: {project["id"]}', align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.ln(10)

    # Executive Summary
    pdf.chapter_title('Executive Summary')
    pdf.body_text(f'Comprehensive analytics report for {project["name"]}. {total_events:,} events captured across {unique_users:,} unique users and {sessions:,} sessions in the last 7 days.')
    pdf.ln(3)

    # Web Analytics Overview
    pdf.chapter_title('1. Web Analytics Overview')
    pdf.add_table(
        ['Metric', 'Value'],
        [
            ['Total Events', f'{total_events:,}'],
            ['Unique Users', f'{unique_users:,}'],
            ['Sessions', f'{sessions:,}'],
            ['Avg Events/Session', str(avg_events_session)]
        ],
        [95, 95]
    )

    # Event Breakdown
    if data.get("events"):
        pdf.section_title('Event Breakdown')
        event_data = []
        for event, count in data.get("events", [])[:8]:
            pct = round(count / total_events * 100, 1) if total_events > 0 else 0
            event_data.append([event, f'{count:,}', f'{pct}%'])
        pdf.add_table(['Event Type', 'Count', '% of Total'], event_data, [80, 55, 55])

    # Web Vitals
    pdf.chapter_title('2. Web Vitals Performance')
    pdf.add_metric_box('LCP (Largest Contentful Paint)', f'{lcp:,.0f}ms', rate_web_vital("lcp", lcp))
    pdf.add_metric_box('CLS (Cumulative Layout Shift)', f'{cls_val:.3f}', rate_web_vital("cls", cls_val))
    pdf.add_metric_box('INP (Interaction to Next Paint)', f'{inp:,.0f}ms', rate_web_vital("inp", inp))
    pdf.ln(5)

    # Top Pages
    pdf.add_page()
    pdf.chapter_title('3. Top Pages')
    if data.get("top_pages"):
        page_data = []
        for i, (page, views) in enumerate(data.get("top_pages", [])[:10], 1):
            page_display = page if page else "(not set)"
            if len(str(page_display)) > 35:
                page_display = str(page_display)[:32] + "..."
            page_data.append([str(i), page_display, f'{views:,}'])
        pdf.add_table(['Rank', 'Page', 'Views'], page_data, [20, 130, 40])

    # Traffic Sources
    pdf.chapter_title('4. Traffic Sources')
    if data.get("traffic_sources"):
        traffic_data = []
        for source, count in data.get("traffic_sources", [])[:8]:
            source_display = source if source else "Direct"
            traffic_data.append([source_display, f'{count:,}'])
        pdf.add_table(['Source', 'Count'], traffic_data, [120, 70])

    # Geographic Distribution
    pdf.chapter_title('5. Geographic Distribution')
    if data.get("geo"):
        geo_data = []
        for country, count in data.get("geo", [])[:8]:
            country_display = country if country else "(unknown)"
            geo_data.append([country_display, f'{count:,}'])
        pdf.add_table(['Country', 'Events'], geo_data, [120, 70])

    # Device Breakdown
    pdf.chapter_title('6. Device Breakdown')
    if data.get("devices"):
        device_data = []
        for device, count in data.get("devices", []):
            device_display = device if device else "(unknown)"
            pct = round(count / total_events * 100, 1) if total_events > 0 else 0
            device_data.append([device_display, f'{count:,}', f'{pct}%'])
        pdf.add_table(['Device', 'Count', '% of Total'], device_data, [70, 60, 60])

    # Daily Trend
    pdf.chapter_title('7. Daily Trend (Last 7 Days)')
    if data.get("daily_trend"):
        trend_data = []
        for date, events, users in data.get("daily_trend", []):
            trend_data.append([str(date), f'{events:,}', f'{users:,}'])
        pdf.add_table(['Date', 'Events', 'Users'], trend_data, [70, 60, 60])

    # Save PDF
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    pdf_filename = f"{project['domain'].replace('.', '_')}_report_{datetime.now().strftime('%Y%m%d')}.pdf"
    pdf_path = REPORTS_DIR / pdf_filename
    pdf.output(str(pdf_path))

    return str(pdf_path)


def generate_summary_pdf(summaries: list) -> str:
    """Generate summary PDF for all projects - Director Rule: MANDATORY"""
    if not PDF_AVAILABLE:
        print("WARNING: PDF generation skipped - fpdf not available")
        return None

    today = datetime.now().strftime("%B %d, %Y")

    total_events = sum(s["total_events"] for s in summaries)
    total_users = sum(s["unique_users"] for s in summaries)
    total_sessions = sum(s["sessions"] for s in summaries)

    pdf = AnalyticsPDF()
    pdf.add_page()

    # Title
    pdf.set_font('Helvetica', 'B', 20)
    pdf.set_text_color(41, 128, 185)
    pdf.cell(0, 15, 'PostHog Analytics Summary', align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.set_font('Helvetica', '', 12)
    pdf.set_text_color(0, 0, 0)
    pdf.cell(0, 8, 'All Projects - Last 7 Days', align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.cell(0, 6, f'Report Date: {today}', align='C', new_x="LMARGIN", new_y="NEXT")
    pdf.ln(10)

    # Executive Summary
    pdf.chapter_title('Executive Summary')
    pdf.body_text('Cross-project analytics summary for Paradise Media owned properties tracked in PostHog.')
    pdf.ln(3)

    # Aggregate stats
    pdf.section_title('Aggregate Totals (Last 7 Days)')
    pdf.add_table(
        ['Metric', 'Value'],
        [
            ['Total Events', f'{total_events:,}'],
            ['Total Unique Users', f'{total_users:,}'],
            ['Total Sessions', f'{total_sessions:,}']
        ],
        [95, 95]
    )

    # Project Comparison
    pdf.chapter_title('Project Comparison')
    project_data = []
    for s in summaries:
        lcp = s["web_vitals"].get("lcp", 0)
        lcp_rating = rate_web_vital("lcp", lcp)
        short_rating = "Good" if lcp_rating == "Good" else "Needs Imp." if lcp_rating == "Needs Improvement" else lcp_rating
        project_data.append([
            s['project'],
            f'{s["total_events"]:,}',
            f'{s["unique_users"]:,}',
            f'{s["sessions"]:,}',
            f'{lcp:.0f}ms ({short_rating})'
        ])
    pdf.add_table(
        ['Project', 'Events', 'Users', 'Sessions', 'LCP'],
        project_data,
        [45, 35, 30, 30, 50]
    )

    # Projects without data
    pdf.chapter_title('Projects Not Reporting Data')
    pdf.add_table(
        ['Project', 'Project ID', 'Status'],
        [
            ['europeangaming.eu', '290042', 'No data ingested'],
            ['esports.gg', '291582', 'No data ingested'],
            ['dotesports.com', '291573', 'No data ingested']
        ],
        [70, 50, 70]
    )
    pdf.body_text('Recommendation: Verify PostHog tracking implementation on these domains.')

    # Save PDF
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    pdf_path = REPORTS_DIR / f"SUMMARY_ALL_PROJECTS_{datetime.now().strftime('%Y%m%d')}.pdf"
    pdf.output(str(pdf_path))

    return str(pdf_path)


# All available projects
ALL_PROJECTS = {
    "lover.io": {"id": 290016, "name": "lover.io", "domain": "lover.io"},
    "northeasttimes.com": {"id": 290039, "name": "Northeastimes.com", "domain": "northeasttimes.com"},
    "pokerology.com": {"id": 266520, "name": "Pokerology.com", "domain": "pokerology.com"},
    "europeangaming.eu": {"id": 290042, "name": "europeangaming.eu", "domain": "europeangaming.eu"},
    "esports.gg": {"id": 291582, "name": "Esports.gg", "domain": "esports.gg"},
    "dotesports.com": {"id": 291573, "name": "Dotesports.com", "domain": "dotesports.com"},
}


def posthog_query(project_id: int, query: str) -> dict:
    """Execute a HogQL query against PostHog API"""
    url = f"{POSTHOG_HOST}/api/projects/{project_id}/query/"
    headers = {
        "Authorization": f"Bearer {POSTHOG_PERSONAL_API_KEY}",
        "Content-Type": "application/json"
    }
    data = json.dumps({"query": {"kind": "HogQLQuery", "query": query}}).encode("utf-8")

    req = urllib.request.Request(url, data=data, headers=headers, method="POST")
    try:
        with urllib.request.urlopen(req, timeout=60) as response:
            return json.loads(response.read().decode())
    except urllib.error.HTTPError as e:
        print(f"PostHog API error for project {project_id}: {e.code}")
        return {"results": [], "error": str(e)}
    except Exception as e:
        print(f"Error querying project {project_id}: {e}")
        return {"results": [], "error": str(e)}


def fetch_all_metrics(project_id: int) -> dict:
    """Fetch all analytics metrics for a project"""
    data = {}

    # Overall stats
    result = posthog_query(project_id, """
        SELECT
            count() as total_events,
            uniqExact(distinct_id) as unique_users,
            uniqExact(properties.$session_id) as sessions
        FROM events
        WHERE timestamp >= now() - INTERVAL 7 DAY
    """)
    if result.get("results"):
        row = result["results"][0]
        data["total_events"] = row[0]
        data["unique_users"] = row[1]
        data["sessions"] = row[2]

    # Event breakdown
    result = posthog_query(project_id, """
        SELECT event, count() as count
        FROM events
        WHERE timestamp >= now() - INTERVAL 7 DAY
        GROUP BY event
        ORDER BY count DESC
        LIMIT 10
    """)
    data["events"] = result.get("results", [])

    # Top pages
    result = posthog_query(project_id, """
        SELECT properties.$pathname as page, count() as views
        FROM events
        WHERE event = '$pageview' AND timestamp >= now() - INTERVAL 7 DAY
        GROUP BY page
        ORDER BY views DESC
        LIMIT 15
    """)
    data["top_pages"] = result.get("results", [])

    # Web vitals
    result = posthog_query(project_id, """
        SELECT
            avg(properties.$web_vitals_LCP_value) as avg_LCP,
            avg(properties.$web_vitals_CLS_value) as avg_CLS,
            avg(properties.$web_vitals_INP_value) as avg_INP
        FROM events
        WHERE event = '$web_vitals' AND timestamp >= now() - INTERVAL 7 DAY
    """)
    if result.get("results") and result["results"][0]:
        row = result["results"][0]
        data["web_vitals"] = {
            "lcp": row[0] or 0,
            "cls": row[1] or 0,
            "inp": row[2] or 0
        }
    else:
        data["web_vitals"] = {"lcp": 0, "cls": 0, "inp": 0}

    # Scroll depth
    result = posthog_query(project_id, """
        SELECT
            avg(properties.$prev_pageview_max_scroll_percentage) as avg_max_scroll,
            avg(properties.$prev_pageview_last_scroll_percentage) as avg_last_scroll,
            count() as pageleave_count
        FROM events
        WHERE event = '$pageleave' AND timestamp >= now() - INTERVAL 7 DAY
    """)
    if result.get("results") and result["results"][0]:
        row = result["results"][0]
        data["scroll_depth"] = {
            "avg_max_scroll": row[0] or 0,
            "avg_last_scroll": row[1] or 0,
            "pageleave_count": row[2] or 0
        }
    else:
        data["scroll_depth"] = {"avg_max_scroll": 0, "avg_last_scroll": 0, "pageleave_count": 0}

    # Traffic sources
    result = posthog_query(project_id, """
        SELECT properties.$referring_domain as referrer, count() as count
        FROM events
        WHERE event = '$pageview' AND timestamp >= now() - INTERVAL 7 DAY
        GROUP BY referrer
        ORDER BY count DESC
        LIMIT 10
    """)
    data["traffic_sources"] = result.get("results", [])

    # Geographic distribution
    result = posthog_query(project_id, """
        SELECT properties.$geoip_country_name as country, count() as count
        FROM events
        WHERE timestamp >= now() - INTERVAL 7 DAY
        GROUP BY country
        ORDER BY count DESC
        LIMIT 10
    """)
    data["geo"] = result.get("results", [])

    # Device breakdown
    result = posthog_query(project_id, """
        SELECT properties.$device_type as device, count() as count
        FROM events
        WHERE timestamp >= now() - INTERVAL 7 DAY
        GROUP BY device
        ORDER BY count DESC
    """)
    data["devices"] = result.get("results", [])

    # Browser breakdown
    result = posthog_query(project_id, """
        SELECT properties.$browser as browser, count() as count
        FROM events
        WHERE timestamp >= now() - INTERVAL 7 DAY
        GROUP BY browser
        ORDER BY count DESC
        LIMIT 5
    """)
    data["browsers"] = result.get("results", [])

    # Daily trend
    result = posthog_query(project_id, """
        SELECT
            toDate(timestamp) as date,
            count() as events,
            uniqExact(distinct_id) as users
        FROM events
        WHERE timestamp >= now() - INTERVAL 7 DAY
        GROUP BY date
        ORDER BY date DESC
    """)
    data["daily_trend"] = result.get("results", [])

    return data


def rate_web_vital(metric: str, value: float) -> str:
    """Rate web vital metrics"""
    if value == 0:
        return "No Data"
    if metric == "lcp":
        if value <= 1200: return "Good"
        elif value <= 2500: return "Needs Improvement"
        else: return "Poor"
    elif metric == "cls":
        if value <= 0.1: return "Good"
        elif value <= 0.25: return "Needs Improvement"
        else: return "Poor"
    elif metric == "inp":
        if value <= 200: return "Good"
        elif value <= 500: return "Needs Improvement"
        else: return "Poor"
    return "Unknown"


def generate_markdown_report(project: dict, data: dict) -> str:
    """Generate markdown report for a project"""
    today = datetime.now().strftime("%B %d, %Y")

    total_events = data.get("total_events", 0)
    unique_users = data.get("unique_users", 0)
    sessions = data.get("sessions", 0)
    avg_events_session = round(total_events / sessions, 1) if sessions > 0 else 0

    web_vitals = data.get("web_vitals", {"lcp": 0, "cls": 0, "inp": 0})
    lcp = round(web_vitals.get("lcp", 0), 0)
    cls_val = round(web_vitals.get("cls", 0), 3)
    inp = round(web_vitals.get("inp", 0), 0)

    scroll_depth = data.get("scroll_depth", {})
    avg_max_scroll = round(scroll_depth.get("avg_max_scroll", 0), 1)
    avg_last_scroll = round(scroll_depth.get("avg_last_scroll", 0), 1)

    # Event breakdown
    event_rows = ""
    for event, count in data.get("events", []):
        pct = round(count / total_events * 100, 1) if total_events > 0 else 0
        event_rows += f"| {event} | {count:,} | {pct}% |\n"

    # Top pages
    page_rows = ""
    for i, (page, views) in enumerate(data.get("top_pages", [])[:10], 1):
        page_display = page if page else "(not set)"
        if len(str(page_display)) > 50:
            page_display = str(page_display)[:47] + "..."
        page_rows += f"| {i} | {page_display} | {views:,} |\n"

    # Traffic sources
    traffic_rows = ""
    for source, count in data.get("traffic_sources", []):
        source_display = source if source else "Direct"
        traffic_rows += f"| {source_display} | {count:,} |\n"

    # Geographic distribution
    geo_rows = ""
    for country, count in data.get("geo", []):
        country_display = country if country else "(unknown)"
        geo_rows += f"| {country_display} | {count:,} |\n"

    # Device breakdown
    device_rows = ""
    for device, count in data.get("devices", []):
        device_display = device if device else "(unknown)"
        pct = round(count / total_events * 100, 1) if total_events > 0 else 0
        device_rows += f"| {device_display} | {count:,} | {pct}% |\n"

    # Daily trend
    trend_rows = ""
    for date, events, users in data.get("daily_trend", []):
        trend_rows += f"| {date} | {events:,} | {users:,} |\n"

    report = f"""# PostHog Analytics Report - {project['name']}

**Domain:** {project['domain']}
**Report Date:** {today}
**Period:** Last 7 Days
**Project ID:** {project['id']}
**Report ID:** PHOG-{project['domain'].upper().replace('.', '-')}-{datetime.now().strftime('%Y%m%d')}

---

## Executive Summary

Comprehensive analytics report for {project['name']}. {total_events:,} events captured across {unique_users:,} unique users and {sessions:,} sessions in the last 7 days.

---

## 1. Web Analytics Overview

| Metric | Value |
|--------|-------|
| Total Events | {total_events:,} |
| Unique Users | {unique_users:,} |
| Sessions | {sessions:,} |
| Avg Events/Session | {avg_events_session} |

### Event Breakdown
| Event Type | Count | % of Total |
|------------|-------|------------|
{event_rows}

---

## 2. Web Vitals Performance

| Metric | Value | Rating |
|--------|-------|--------|
| LCP (Largest Contentful Paint) | {lcp:,.0f}ms | {rate_web_vital("lcp", lcp)} |
| CLS (Cumulative Layout Shift) | {cls_val:.3f} | {rate_web_vital("cls", cls_val)} |
| INP (Interaction to Next Paint) | {inp:,.0f}ms | {rate_web_vital("inp", inp)} |

### Web Vitals Thresholds Reference
- **LCP:** Good ≤1200ms, Needs Improvement ≤2500ms, Poor >2500ms
- **CLS:** Good ≤0.1, Needs Improvement ≤0.25, Poor >0.25
- **INP:** Good ≤200ms, Needs Improvement ≤500ms, Poor >500ms

---

## 3. Scroll Depth Analysis

| Metric | Value | Rating |
|--------|-------|--------|
| Avg Max Scroll | {avg_max_scroll}% | {"Good" if avg_max_scroll >= 50 else "Needs Improvement" if avg_max_scroll > 0 else "No Data"} |
| Avg Last Scroll | {avg_last_scroll}% | {"Good" if avg_last_scroll >= 40 else "Needs Improvement" if avg_last_scroll > 0 else "No Data"} |
| Page Leaves Tracked | {scroll_depth.get('pageleave_count', 0):,} | - |

---

## 4. Top Pages

| Rank | Page | Views |
|------|------|-------|
{page_rows}

---

## 5. Traffic Sources

| Source | Count |
|--------|-------|
{traffic_rows}

---

## 6. Geographic Distribution

| Country | Events |
|---------|--------|
{geo_rows}

---

## 7. Device Breakdown

| Device | Count | % of Total |
|--------|-------|------------|
{device_rows}

---

## 8. Daily Trend (Last 7 Days)

| Date | Events | Users |
|------|--------|-------|
{trend_rows}

---

## Report Generated By

**BlackTeam - DataViz + Insight**
**Date:** {today}
**Command:** /posthog_analysis

---

*This report was auto-generated using PostHog API data.*
"""
    return report


def generate_summary_report(summaries: list) -> str:
    """Generate a summary report across all projects"""
    today = datetime.now().strftime("%B %d, %Y")

    total_events = sum(s["total_events"] for s in summaries)
    total_users = sum(s["unique_users"] for s in summaries)
    total_sessions = sum(s["sessions"] for s in summaries)

    project_rows = ""
    for s in summaries:
        lcp = s["web_vitals"].get("lcp", 0)
        lcp_rating = rate_web_vital("lcp", lcp)
        project_rows += f"| {s['project']} | {s['total_events']:,} | {s['unique_users']:,} | {s['sessions']:,} | {lcp:.0f}ms ({lcp_rating}) |\n"

    report = f"""# PostHog Analytics Summary - All Projects

**Report Date:** {today}
**Period:** Last 7 Days
**Generated By:** BlackTeam - DataViz + Insight
**Command:** /posthog_analysis run all

---

## Executive Summary

Cross-project analytics summary for Paradise Media owned properties tracked in PostHog.

**Aggregate Totals (Last 7 Days):**
- **Total Events:** {total_events:,}
- **Total Unique Users:** {total_users:,}
- **Total Sessions:** {total_sessions:,}

---

## Project Comparison

| Project | Events | Users | Sessions | LCP (Rating) |
|---------|--------|-------|----------|--------------|
{project_rows}

---

## Individual Report Links

"""

    for s in summaries:
        report += f"- **{s['project']}:** `{s['filepath']}`\n"

    report += f"""

---

## Projects Not Reporting Data

The following projects are configured but have not ingested any events:

| Project | Project ID | Status |
|---------|-----------|--------|
| europeangaming.eu | 290042 | No data ingested |
| esports.gg | 291582 | No data ingested |
| dotesports.com | 291573 | No data ingested |

**Recommendation:** Verify PostHog tracking implementation on these domains.

---

## Next Steps

1. Review individual project reports for detailed insights
2. Investigate projects without data
3. Set up automated daily reporting if needed

---

*Report generated on {today} by BlackTeam*
"""

    return report


def process_single_domain(domain: str):
    """Process a single domain and generate report"""
    domain_lower = domain.lower()

    if domain_lower not in ALL_PROJECTS:
        print(f"ERROR: Unknown domain '{domain}'")
        print("\nAvailable domains:")
        for d in ALL_PROJECTS.keys():
            print(f"  - {d}")
        return 1

    project = ALL_PROJECTS[domain_lower]

    print(f"\n{'='*60}")
    print(f"PostHog Analytics Report - {project['name']}")
    print(f"{'='*60}")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Project ID: {project['id']}")

    # Fetch metrics
    print("\nFetching analytics data...")
    data = fetch_all_metrics(project["id"])

    if not data.get("total_events"):
        print(f"\nWARNING: No events found for {project['name']}")
        print("This project may not have PostHog tracking implemented yet.")
        return 1

    # Display quick summary
    print(f"\n--- Quick Summary ---")
    print(f"Total Events: {data['total_events']:,}")
    print(f"Unique Users: {data['unique_users']:,}")
    print(f"Sessions: {data['sessions']:,}")

    web_vitals = data.get("web_vitals", {})
    lcp = web_vitals.get("lcp", 0)
    cls_val = web_vitals.get("cls", 0)
    inp = web_vitals.get("inp", 0)

    print(f"\n--- Web Vitals ---")
    print(f"LCP: {lcp:,.0f}ms ({rate_web_vital('lcp', lcp)})")
    print(f"CLS: {cls_val:.3f} ({rate_web_vital('cls', cls_val)})")
    print(f"INP: {inp:,.0f}ms ({rate_web_vital('inp', inp)})")

    # Generate and save report
    print("\nGenerating report...")
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    # Markdown report
    report = generate_markdown_report(project, data)
    filename = f"{project['domain'].replace('.', '_')}_report_{datetime.now().strftime('%Y%m%d')}.md"
    filepath = REPORTS_DIR / filename
    filepath.write_text(report)
    print(f"Markdown saved: {filepath}")

    # PDF report (Director Rule: MANDATORY)
    pdf_path = generate_pdf_report(project, data)
    if pdf_path:
        print(f"PDF saved: {pdf_path}")

    print(f"\n{'='*60}")

    return 0


def process_all_projects():
    """Generate reports for all projects with data"""
    print(f"\n{'='*60}")
    print("PostHog All Projects Report Generator")
    print(f"{'='*60}")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Create reports directory
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    all_summaries = []

    # Only process projects known to have data
    active_domains = ["lover.io", "northeasttimes.com", "pokerology.com"]

    for domain in active_domains:
        project = ALL_PROJECTS[domain]
        print(f"\n{'='*40}")
        print(f"Processing: {project['name']} (ID: {project['id']})")
        print(f"{'='*40}")

        # Fetch metrics
        print("Fetching analytics data...")
        data = fetch_all_metrics(project["id"])

        if not data.get("total_events"):
            print(f"WARNING: No events found for {project['name']}")
            continue

        print(f"  - Total Events: {data['total_events']:,}")
        print(f"  - Unique Users: {data['unique_users']:,}")
        print(f"  - Sessions: {data['sessions']:,}")

        # Generate markdown report
        print("Generating report...")
        report = generate_markdown_report(project, data)

        # Save markdown report
        filename = f"{project['domain'].replace('.', '_')}_report_{datetime.now().strftime('%Y%m%d')}.md"
        filepath = REPORTS_DIR / filename
        filepath.write_text(report)
        print(f"Markdown saved: {filepath}")

        # Generate PDF report (Director Rule: MANDATORY)
        pdf_path = generate_pdf_report(project, data)
        if pdf_path:
            print(f"PDF saved: {pdf_path}")

        # Collect summary
        all_summaries.append({
            "project": project["name"],
            "domain": project["domain"],
            "project_id": project["id"],
            "total_events": data["total_events"],
            "unique_users": data["unique_users"],
            "sessions": data["sessions"],
            "web_vitals": data.get("web_vitals", {}),
            "filepath": str(filepath)
        })

    # Generate summary report
    if all_summaries:
        print("\n" + "="*40)
        print("Generating Summary Report")
        print("="*40)

        # Markdown summary
        summary_report = generate_summary_report(all_summaries)
        summary_path = REPORTS_DIR / f"SUMMARY_ALL_PROJECTS_{datetime.now().strftime('%Y%m%d')}.md"
        summary_path.write_text(summary_report)
        print(f"Markdown saved: {summary_path}")

        # PDF summary (Director Rule: MANDATORY)
        pdf_summary_path = generate_summary_pdf(all_summaries)
        if pdf_summary_path:
            print(f"PDF saved: {pdf_summary_path}")

    print(f"\n{'='*60}")
    print("All reports generated successfully!")
    print(f"{'='*60}")

    # Print final summary
    print("\nProjects Analyzed:")
    for s in all_summaries:
        print(f"  ✅ {s['project']} - {s['total_events']:,} events, {s['unique_users']:,} users")

    print("\nProjects Without Data:")
    for domain in ["europeangaming.eu", "esports.gg", "dotesports.com"]:
        print(f"  ⏳ {domain} - No data ingested")

    print(f"\nReports saved to: {REPORTS_DIR}")

    return 0


def main():
    """Main execution - handle command line arguments"""

    if not POSTHOG_PERSONAL_API_KEY:
        print("ERROR: POSTHOG_PERSONAL_API_KEY not set")
        print("Run: export POSTHOG_PERSONAL_API_KEY=$(grep POSTHOG_PERSONAL_API_KEY /home/andre/.keys/.env | cut -d'=' -f2)")
        return 1

    # Parse arguments
    args = sys.argv[1:]

    if not args or args[0].lower() == "all":
        # No arguments or "all" - process all projects
        return process_all_projects()
    else:
        # Single domain specified
        domain = args[0]
        return process_single_domain(domain)


if __name__ == "__main__":
    exit(main())
